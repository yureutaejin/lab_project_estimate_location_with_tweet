{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6903eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "\n",
    "import os\n",
    "\n",
    "from rake_nltk import Rake\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38167675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words for preprocessing\n",
    "# %%\n",
    "stop_words = list([\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\", \"A\", \"About\", \"Above\", \"Across\", \"After\", \"Afterwards\", \"Again\", \"Against\",\n",
    "    \"All\", \"Almost\", \"Alone\", \"Along\", \"Already\", \"Also\", \"Although\", \"Always\",\n",
    "    \"Am\", \"Among\", \"Amongst\", \"Amoungst\", \"Amount\", \"An\", \"And\", \"Another\",\n",
    "    \"Any\", \"Anyhow\", \"Anyone\", \"Anything\", \"Anyway\", \"Anywhere\", \"Are\",\n",
    "    \"Around\", \"As\", \"At\", \"Back\", \"Be\", \"Became\", \"Because\", \"Become\",\n",
    "    \"Becomes\", \"Becoming\", \"Been\", \"Before\", \"Beforehand\", \"Behind\", \"Being\",\n",
    "    \"Below\", \"Beside\", \"Besides\", \"Between\", \"Beyond\", \"Bill\", \"Both\",\n",
    "    \"Bottom\", \"But\", \"By\", \"Call\", \"Can\", \"Cannot\", \"Cant\", \"Co\", \"Con\",\n",
    "    \"Could\", \"Couldnt\", \"Cry\", \"De\", \"Describe\", \"Detail\", \"Do\", \"Done\",\n",
    "    \"Down\", \"Due\", \"During\", \"Each\", \"Eg\", \"Eight\", \"Either\", \"Eleven\", \"Else\",\n",
    "    \"Elsewhere\", \"Empty\", \"Enough\", \"Etc\", \"Even\", \"Ever\", \"Every\", \"Everyone\",\n",
    "    \"Everything\", \"Everywhere\", \"Except\", \"Few\", \"Fifteen\", \"Fifty\", \"Fill\",\n",
    "    \"Find\", \"Fire\", \"First\", \"Five\", \"For\", \"Former\", \"Formerly\", \"Forty\",\n",
    "    \"Found\", \"Four\", \"From\", \"Front\", \"Full\", \"Further\", \"get\", \"give\", \"go\",\n",
    "    \"Had\", \"Has\", \"Hasnt\", \"Have\", \"He\", \"Hence\", \"Her\", \"Here\", \"Hereafter\",\n",
    "    \"Hereby\", \"Herein\", \"Hereupon\", \"Hers\", \"Herself\", \"Him\", \"Himself\", \"His\",\n",
    "    \"How\", \"However\", \"Hundred\", \"I\", \"Ie\", \"If\", \"In\", \"Inc\", \"Indeed\",\n",
    "    \"Interest\", \"Into\", \"Is\", \"It\", \"Its\", \"Itself\", \"Keep\", \"Last\", \"Latter\",\n",
    "    \"Latterly\", \"Least\", \"Less\", \"Ltd\", \"Made\", \"Many\", \"May\", \"Me\",\n",
    "    \"Meanwhile\", \"Might\", \"Mill\", \"Mine\", \"More\", \"Moreover\", \"Most\", \"Mostly\",\n",
    "    \"Move\", \"Much\", \"Must\", \"My\", \"Myself\", \"Name\", \"Namely\", \"Neither\",\n",
    "    \"Never\", \"Nevertheless\", \"Next\", \"Nine\", \"No\", \"Nobody\", \"None\", \"Noone\",\n",
    "    \"Nor\", \"Not\", \"Nothing\", \"Now\", \"Nowhere\", \"Of\", \"Off\", \"Often\", \"On\",\n",
    "    \"Once\", \"One\", \"Only\", \"Onto\", \"Or\", \"Other\", \"Others\", \"Otherwise\", \"Our\",\n",
    "    \"Ours\", \"Ourselves\", \"Out\", \"Over\", \"Own\", \"Part\", \"Per\", \"Perhaps\",\n",
    "    \"Please\", \"Put\", \"Rather\", \"Re\", \"Same\", \"See\", \"Seem\", \"Seemed\",\n",
    "    \"Seeming\", \"Seems\", \"Serious\", \"Several\", \"She\", \"Should\", \"Show\", \"Side\",\n",
    "    \"Since\", \"Sincere\", \"Six\", \"Sixty\", \"So\", \"Some\", \"Somehow\", \"Someone\",\n",
    "    \"Something\", \"Sometime\", \"SometimeS\", \"Somewhere\", \"Still\", \"Such\",\n",
    "    \"System\", \"Take\", \"Ten\", \"Than\", \"That\", \"The\", \"Their\", \"Them\",\n",
    "    \"Themselves\", \"Then\", \"Thence\", \"There\", \"Thereafter\", \"Thereby\",\n",
    "    \"Therefore\", \"Therein\", \"Thereupon\", \"These\", \"They\", \"Thick\", \"Thin\",\n",
    "    \"Third\", \"This\", \"Those\", \"Though\", \"Three\", \"Through\", \"Throughout\",\n",
    "    \"Thru\", \"Thus\", \"To\", \"Together\", \"Too\", \"Top\", \"Toward\", \"Towards\",\n",
    "    \"Twelve\", \"Twenty\", \"Two\", \"Un\", \"Under\", \"Until\", \"Up\", \"Upon\", \"Us\",\n",
    "    \"Very\", \"Via\", \"Was\", \"We\", \"Well\", \"Were\", \"What\", \"Whatever\", \"When\",\n",
    "    \"Whence\", \"Whenever\", \"Where\", \"Whereafter\", \"Whereas\", \"Whereby\",\n",
    "    \"Wherein\", \"Whereupon\", \"Wherever\", \"Whether\", \"Which\", \"While\", \"Whither\",\n",
    "    \"Who\", \"Whoever\", \"Whole\", \"Whom\", \"Whose\", \"Why\", \"Will\", \"With\",\n",
    "    \"Within\", \"Without\", \"Would\", \"Yet\", \"You\", \"Your\", \"Yours\", \"Yourself\",\n",
    "    \"Yourselves\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b8a9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "\n",
    "# %% keywords split해서 하나로 합쳐주기\n",
    "\n",
    "def listsum(inputlist):\n",
    "    results = []\n",
    "    listlen = len(inputlist)\n",
    "    for i in range(listlen):\n",
    "        temp = inputlist[i].split(\" \")\n",
    "        for j in range(len(temp)):\n",
    "            if (len(temp[j]) > 2):\n",
    "                results.append(temp[j])\n",
    "\n",
    "    results = list(dict.fromkeys(results))\n",
    "    return results\n",
    "\n",
    "# %% 수정본 , rank 값이 1보다 큰 애들 적용\n",
    "\n",
    "def extractor(newsset, stop_words):\n",
    "    # stop_words = stopwords.words('english')\n",
    "    r1 = Rake(stopwords=stop_words)\n",
    "    # r2 = Rake()\n",
    "    title = newsset['Title']\n",
    "    title = preprocess_news(title)\n",
    "    news = newsset['text']\n",
    "    news = preprocess_news(news)\n",
    "    \n",
    "    # date = newsset['Date']\n",
    "    # keyword = []\n",
    "    r1.extract_keywords_from_text(title)\n",
    "    r1.get_ranked_phrases()\n",
    "    title_scores = r1.get_ranked_phrases_with_scores()\n",
    "\n",
    "    r1.extract_keywords_from_text(news)\n",
    "    r1.get_ranked_phrases()\n",
    "    text_scores = r1.get_ranked_phrases_with_scores()\n",
    "\n",
    "    # keyword.append(news_scores[i][1])\n",
    "    # keyword.append(title_scores[i][1])\n",
    "    title_tp = []\n",
    "    text_tp = []\n",
    "    new_text_scores = text_scores[:10]  # int(len(text_scores)/10)]\n",
    "    for data in new_text_scores:\n",
    "        text_tp.append(data[1])\n",
    "\n",
    "    for i in range(len(title_scores)):\n",
    "        if ((title_scores[i][0]) > 1):\n",
    "            title_tp.append(title_scores[i][1])\n",
    "\n",
    "    # return title_tp,text_tp\n",
    "    return listsum(title_tp), listsum(text_tp)\n",
    "\n",
    "# %% keyword extract from title, text no rank\n",
    "\n",
    "def extractor_norank(newsset, stop_words):\n",
    "    # stop_words = stopwords.words('english')\n",
    "    r1 = Rake(stopwords=stop_words)\n",
    "    # r2 = Rake()\n",
    "    title = newsset['Title']\n",
    "    # title = preprocess_news(title)\n",
    "    news = newsset['text']\n",
    "    # news = preprocess_news(news)\n",
    "\n",
    "    # date = newsset['Date']\n",
    "    # keyword = []\n",
    "    r1.extract_keywords_from_text(title)\n",
    "    r1.get_ranked_phrases()\n",
    "    title_scores = r1.get_ranked_phrases_with_scores()\n",
    "\n",
    "    r1.extract_keywords_from_text(news)\n",
    "    r1.get_ranked_phrases()\n",
    "    text_scores = r1.get_ranked_phrases_with_scores()\n",
    "    # keyword.append(news_scores[i][1])\n",
    "    # keyword.append(title_scores[i][1])\n",
    "    \n",
    "    title_tp = []\n",
    "    text_tp = []\n",
    "    \n",
    "#     print(\"start\")\n",
    "    for i in range(len(title_scores)):\n",
    "\n",
    "# # 오류났을 떼\n",
    "#         print(text_scores[i][1])\n",
    "        \n",
    "        \n",
    "        # 랭킹 메기는 것 없앰\n",
    "        title_tp.append(title_scores[i][1])\n",
    "        text_tp.append(text_scores[i][1])\n",
    "\n",
    "    # return title_tp,text_tp\n",
    "#     print(\"finish\\n\")\n",
    "    return listsum(title_tp), listsum(text_tp)\n",
    "\n",
    "# %%\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "                      u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                      u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                      u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                      u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                      u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U000024C2-\\U0001F251\"\n",
    "                      u\"\\U0001f926-\\U0001f937\"\n",
    "                      u\"\\U00010000-\\U0010ffff\"\n",
    "                      u\"\\u2640-\\u2642\"\n",
    "                      u\"\\u2600-\\u2B55\"\n",
    "                      u\"\\u200d\"\n",
    "                      u\"\\u23cf\"\n",
    "                      u\"\\u23e9\"\n",
    "                      u\"\\u231a\"\n",
    "                      u\"\\ufe0f\"  # dingbats\n",
    "                      u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "\n",
    "# %%\n",
    "def preprocess_tweet(text):\n",
    "    # convert text to lower-case\n",
    "    nopunc = text.lower()\n",
    "    nopunc = re.sub(\"\\\\’\", \"'\", nopunc)\n",
    "    nopunc = re.sub(\"\\\\'s\", \"\", nopunc)\n",
    "    nopunc = re.sub(\"\\\\'d\", \"\", nopunc)\n",
    "    nopunc = re.sub(\"\\\\'m\", \"\", nopunc)\n",
    "    nopunc = re.sub(\"\\\\'ve\", \"\", nopunc)\n",
    "    nopunc = re.sub(\"\\\\'re\", \"\", nopunc)\n",
    "\n",
    "    # remove URLs\n",
    "    nopunc = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', nopunc)\n",
    "    nopunc = re.sub(r'http\\S+', '', nopunc)\n",
    "    # remove usernames\n",
    "    nopunc = re.sub('@[^\\s]+', '', nopunc)\n",
    "    # remove the # in #hashtag\n",
    "    nopunc = re.sub(r'#([^\\s]+)', r'\\1', nopunc)\n",
    "    # remove rt\n",
    "    nopunc = re.sub('^(rt )', '', nopunc)\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = re.sub(\"  \", \" \", nopunc)\n",
    "\n",
    "    # \\'를 제외했음, it's 같은 '도 생략되어버림.\n",
    "    punctuations = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    result = [char for char in nopunc if char not in punctuations]\n",
    "    # Join the characters again to form the string.\n",
    "    result = ''.join(result)\n",
    "\n",
    "    result = remove_emojis(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "# %%\n",
    "def preprocess_news(inputnews):\n",
    "    inputnews = inputnews.lower()\n",
    "    inputnews = re.sub(\"\\\\'s\", \"\", inputnews)\n",
    "    inputnews = re.sub(\"\\\\'d\", \"\", inputnews)\n",
    "    inputnews = re.sub(\"\\\\'m\", \"\", inputnews)\n",
    "    inputnews = re.sub(\"\\\\'ve\", \"\", inputnews)\n",
    "    inputnews = re.sub(\"\\\\'re\", \"\", inputnews)\n",
    "\n",
    "    inputnews = [char for char in inputnews if char not in string.punctuation]\n",
    "    # Join the characters again to form the string.\n",
    "    inputnews = ''.join(inputnews)\n",
    "\n",
    "    return inputnews\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "def is_word_in_text(word, text):\n",
    "    \"\"\"\n",
    "    Check if a word is in a text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word : str\n",
    "    text : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool : True if word is in text, otherwise False.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> is_word_in_text(\"Python\", \"python is awesome.\")\n",
    "    True\n",
    "\n",
    "    >>> is_word_in_text(\"Python\", \"camelCase is pythonic.\")\n",
    "    False\n",
    "\n",
    "    >>> is_word_in_text(\"Python\", \"At the end is Python\")\n",
    "    True\n",
    "    \"\"\"\n",
    "    pattern = r'(^|[^\\w]){}([^\\w]|$)'.format(word)\n",
    "    pattern = re.compile(pattern, re.IGNORECASE)\n",
    "    matches = re.search(pattern, text)\n",
    "    return bool(matches)\n",
    "\n",
    "#%%\n",
    "def tf_vectorizer(filtered_news, filtered_tw, my_stop_words):\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words = set(my_stop_words))\n",
    "    \n",
    "   \n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([filtered_news,filtered_tw])\n",
    "    return cosine_similarity(tfidf_matrix[0],tfidf_matrix[1])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad60ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load news json file\n",
    "tweet_city_names = ['Boston','San Francisco', 'Detroit', 'Jacksonville', 'Fort Worth']\n",
    "news_city_names = ['Boston', 'San Francisco', 'Detroit', 'Jacksonville', 'Fort Worth']\n",
    "\n",
    "for tweet_city_name in tweet_city_names:\n",
    "    for city_name in news_city_names:\n",
    "\n",
    "\n",
    "\n",
    "        # %%\n",
    "        # with open('데이터수집.전처리코드/Data/CNNtest_9_Boston2018.json', encoding=\"UTF-8\") as newsfile:\n",
    "        with open('/Users/jinyuntae/Desktop/Personal_project/estimate_location_with_tweet/data/news_data/2_result_data/2_2018_CNNtest_{}.json'.format(city_name), encoding=\"UTF-8\") as newsfile:\n",
    "            newsfile = newsfile.read()\n",
    "            news_data = json.loads(newsfile)\n",
    "            newslen = len(news_data['news'])\n",
    "\n",
    "        # make directory\n",
    "        if os.path.isdir(\"../data/analysis_data/2018_{}2{}\".format(tweet_city_name, city_name)) == False:\n",
    "            os.mkdir(\"../data/analysis_data/2018_{}2{}\".format(tweet_city_name, city_name))\n",
    "\n",
    "        # %% without split\n",
    "        resultpd = pd.DataFrame(columns=[\"Text\", \"Date\"])\n",
    "\n",
    "        for i in range(newslen):\n",
    "            title_norank, text_norank = (extractor_norank(news_data[\"news\"][i], stop_words))\n",
    "            title_result, text_result = (extractor(news_data[\"news\"][i], stop_words))\n",
    "            date = news_data[\"news\"][i]['Date']\n",
    "            tempdf = pd.DataFrame(\n",
    "                {\"ExtTitle_no\": [title_norank], 'ExtText_no': [text_norank], \"ExtTitle\": [title_result], \"Date\": date,\n",
    "                 'ExtText': [text_result], \"Title\": news_data[\"news\"][i][\"Title\"], \"Text\": news_data['news'][i]['text']})\n",
    "\n",
    "            resultpd = resultpd.append(tempdf, ignore_index=True)\n",
    "\n",
    "        # newsmerge = resultpd.groupby([\"Date\"]).agg({\"Title\":'sum'}).reset_index()\n",
    "\n",
    "        # %% 상위 10개, 전처리 후\n",
    "        resultpd1 = pd.DataFrame(columns=[\"Text\", \"Date\"])\n",
    "\n",
    "        for i in range(newslen):\n",
    "            title_norank, text_norank = (extractor_norank(news_data[\"news\"][i], stop_words))\n",
    "            title_result, text_result = (extractor(news_data[\"news\"][i], stop_words))\n",
    "            date = news_data[\"news\"][i]['Date']\n",
    "            tempdf = pd.DataFrame(\n",
    "                {\"ExtTitle_no\": [title_norank], 'ExtText_no': [text_norank], \"ExtTitle\": [title_result], \"Date\": date,\n",
    "                 'ExtText': [text_result], \"Title\": news_data[\"news\"][i][\"Title\"], \"Text\": news_data['news'][i]['text']})\n",
    "\n",
    "            resultpd1 = resultpd1.append(tempdf, ignore_index=True)\n",
    "\n",
    "        # newsmerge = resultpd.groupby([\"Date\"]).agg({\"Title\":'sum'}).reset_index()\n",
    "        # %% 상위 10개, 전처리 전\n",
    "        resultpd2 = pd.DataFrame(columns=[\"Text\", \"Date\"])\n",
    "\n",
    "        for i in range(newslen):\n",
    "            title_norank, text_norank = (extractor_norank(news_data[\"news\"][i], stop_words))\n",
    "            title_result, text_result = (extractor(news_data[\"news\"][i], stop_words))\n",
    "            date = news_data[\"news\"][i]['Date']\n",
    "            tempdf = pd.DataFrame(\n",
    "                {\"ExtTitle_no\": [title_norank], 'ExtText_no': [text_norank], \"ExtTitle\": [title_result], \"Date\": date,\n",
    "                 'ExtText': [text_result], \"Title\": news_data[\"news\"][i][\"Title\"], \"Text\": news_data['news'][i]['text']})\n",
    "\n",
    "            resultpd2 = resultpd2.append(tempdf, ignore_index=True)\n",
    "\n",
    "        # %% 상위 10개, 전처리 후, 샌프란\n",
    "        resultpd3 = pd.DataFrame(columns=[\"Text\", \"Date\"])\n",
    "\n",
    "        for i in range(newslen):\n",
    "            title_norank, text_norank = (extractor_norank(news_data[\"news\"][i], stop_words))\n",
    "            title_result, text_result = (extractor(news_data[\"news\"][i], stop_words))\n",
    "            date = news_data[\"news\"][i]['Date']\n",
    "            tempdf = pd.DataFrame(\n",
    "                {\"ExtTitle_no\": [title_norank], 'ExtText_no': [text_norank], \"ExtTitle\": [title_result], \"Date\": date,\n",
    "                 'ExtText': [text_result], \"Title\": news_data[\"news\"][i][\"Title\"], \"Text\": news_data['news'][i]['text']})\n",
    "\n",
    "            resultpd3 = resultpd3.append(tempdf, ignore_index=True)\n",
    "\n",
    "\n",
    "        # newsmerge = resultpd.groupby([\"Date\"]).agg({\"Title\":'sum'}).reset_index()\n",
    "        # %%\n",
    "        # with open('C:/Users/cowgi/Desktop/University/bigbaselab/PredictLocation_TweetAndNews/boston_tweet_more_150_everymonth_golbang.json', encoding=\"UTF-8\") as twfile:\n",
    "        with open('/Users/jinyuntae/Desktop/Personal_project/estimate_location_with_tweet/data/tweet_data/5_result_data/{}_user_filtered_count0.json'.format(tweet_city_name), encoding=\"UTF-8\") as twfile:\n",
    "            twfile = twfile.read()\n",
    "            tw_data = json.loads(twfile)\n",
    "            usernum = len(tw_data['users'])\n",
    "\n",
    "\n",
    "        # %%\n",
    "        wnews = []\n",
    "        wtweets = []\n",
    "        countlist = []\n",
    "        countlist_no = []\n",
    "        keywordlist = []\n",
    "        keywordlist_no = []\n",
    "        wkeys = []\n",
    "        wkeys_no = []\n",
    "        unions = []\n",
    "\n",
    "        # with open('C:/Users/cowgi/Desktop/University/bigbaselab/PredictLocation_TweetAndNews/CNN_result/join_tweet/200708_1.json', 'a' ,encoding='UTF-8') as tf:\n",
    "        #    sys.stdout = tf\n",
    "        for t in range(usernum):\n",
    "            try:\n",
    "                eachuser = tw_data['users'][t]\n",
    "                username = eachuser['id']\n",
    "                usertimeline = eachuser['timeline']\n",
    "                # count = 0\n",
    "\n",
    "                # datecount = 0\n",
    "                # print(\"{\\\"id : \\\"%s\\\":[\"%(username))\n",
    "                for i in range(len(usertimeline)):\n",
    "                    date = usertimeline[i][\"date\"]\n",
    "                    text = preprocess_tweet(usertimeline[i][\"text\"])\n",
    "                    # text = usertimeline[i][\"text\"]\n",
    "\n",
    "                    for j in range(len(resultpd3)):\n",
    "                        if (date == resultpd3._get_value(j, \"Date\")):\n",
    "                            count = 0\n",
    "                            count_no = 0\n",
    "                            keywords = []\n",
    "                            keywords_no = []\n",
    "                            for word in resultpd3._get_value(j, \"ExtText\"):\n",
    "                                if is_word_in_text(word, text):\n",
    "                                    count += 1\n",
    "                                    keywords.append(word)\n",
    "                            for word in resultpd3._get_value(j, \"ExtText_no\"):\n",
    "                                if is_word_in_text(word, text):\n",
    "                                    count_no += 1\n",
    "                                    keywords_no.append(word)\n",
    "\n",
    "                            #  n(A)+n(B)\n",
    "                            unions.append(len(text.split()) + len(resultpd3._get_value(j, \"ExtText\")))\n",
    "                            wnews.append(resultpd3._get_value(j, \"Title\"))\n",
    "                            wtweets.append(text)\n",
    "                            countlist.append(count)\n",
    "                            countlist_no.append(count_no)\n",
    "                            keywordlist.append(keywords)\n",
    "                            keywordlist_no.append(keywords_no)\n",
    "                            wkeys.append(resultpd3._get_value(j, \"ExtText\"))\n",
    "                            wkeys_no.append(resultpd3._get_value(j, \"ExtText_no\"))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "        #%%\n",
    "\n",
    "        Toppd = pd.DataFrame()\n",
    "\n",
    "        Toppd[\"NewsTitle\"] = wnews\n",
    "        Toppd[\"Tweet\"] = wtweets\n",
    "        Toppd[\"howmany\"] = countlist\n",
    "        Toppd[\"matched_kewords\"] = keywordlist\n",
    "        Toppd[\"Extkey\"] = wkeys\n",
    "        Toppd[\"howmany_norank\"] = countlist_no\n",
    "        Toppd[\"matched_kewords_norank\"] = keywordlist_no\n",
    "        Toppd[\"Extkey_norank\"] = wkeys_no\n",
    "        Toppd[\"Union\"] = unions\n",
    "        tmp = []\n",
    "        tmp_no = []\n",
    "        for i in range(len(Toppd[\"Extkey\"])):\n",
    "            tmp.append(len(Toppd[\"Extkey\"][i]))\n",
    "            tmp_no.append(len(Toppd[\"Extkey_norank\"][i]))\n",
    "        Toppd[\"numKeys\"] = tmp\n",
    "        Toppd[\"numKeys_norank\"] = tmp_no\n",
    "        Toppd[\"ratio\"] = Toppd[\"howmany\"]/Toppd[\"numKeys\"]\n",
    "        Toppd[\"ratio_norank\"] = Toppd[\"howmany_norank\"]/Toppd[\"numKeys_norank\"]\n",
    "        Toppdresult = Toppd\n",
    "        #finalresult = Toppdresult.sort_values(by=[\"howmany_norank\",\"numKeys_norank\"],ascending=[False,True])\n",
    "        finalresult = Toppdresult.sort_values(by=[\"howmany\",\"Union\"],ascending=[False,True])\n",
    "        #finalresult = finalresult.sort_values(by=[\"numKeys_norank\"],ascending=True)\n",
    "        finalresult = finalresult.reset_index(drop=True)\n",
    "        finalresult = finalresult[:200]\n",
    "\n",
    "        #finalresult.to_excel(\"C:/Users/cowgi/Desktop/University/bigbaselab/PredictLocation_TweetAndNews/CNN_result/join_tweet/keywordTop200_sanfransisco.xlsx\")\n",
    "        #finalresult.to_excel(\"C:/Users/cowgi/Desktop/University/bigbaselab/keywordTop200_sort_top10rank_nopre.xlsx\")\n",
    "        #finalresult.to_excel(\"C:/Users/cowgi/Desktop/University/bigbaselab/keywordTop200_sort_top10rank_befpre.xlsx\")\n",
    "        finalresult.to_excel(\"/Users/jinyuntae/Desktop/Personal_project/estimate_location_with_tweet/data/analysis_data/2018_{}2{}/keywordTop200_sort_top10rank_aftpre_{}2{}.xlsx\".format(tweet_city_name, city_name, tweet_city_name, city_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
