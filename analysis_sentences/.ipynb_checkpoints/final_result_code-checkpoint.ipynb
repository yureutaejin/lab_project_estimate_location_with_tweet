{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "643c2e76-86cf-4fef-81e6-fe8132b7a8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Library\n",
    "\n",
    "import os\n",
    "\n",
    "from rake_nltk import Rake\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7927774c-9575-459d-983c-0ebdcc8bcfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words for preprocessing\n",
    "# %%\n",
    "stop_words = list([\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\", \"A\", \"About\", \"Above\", \"Across\", \"After\", \"Afterwards\", \"Again\", \"Against\",\n",
    "    \"All\", \"Almost\", \"Alone\", \"Along\", \"Already\", \"Also\", \"Although\", \"Always\",\n",
    "    \"Am\", \"Among\", \"Amongst\", \"Amoungst\", \"Amount\", \"An\", \"And\", \"Another\",\n",
    "    \"Any\", \"Anyhow\", \"Anyone\", \"Anything\", \"Anyway\", \"Anywhere\", \"Are\",\n",
    "    \"Around\", \"As\", \"At\", \"Back\", \"Be\", \"Became\", \"Because\", \"Become\",\n",
    "    \"Becomes\", \"Becoming\", \"Been\", \"Before\", \"Beforehand\", \"Behind\", \"Being\",\n",
    "    \"Below\", \"Beside\", \"Besides\", \"Between\", \"Beyond\", \"Bill\", \"Both\",\n",
    "    \"Bottom\", \"But\", \"By\", \"Call\", \"Can\", \"Cannot\", \"Cant\", \"Co\", \"Con\",\n",
    "    \"Could\", \"Couldnt\", \"Cry\", \"De\", \"Describe\", \"Detail\", \"Do\", \"Done\",\n",
    "    \"Down\", \"Due\", \"During\", \"Each\", \"Eg\", \"Eight\", \"Either\", \"Eleven\", \"Else\",\n",
    "    \"Elsewhere\", \"Empty\", \"Enough\", \"Etc\", \"Even\", \"Ever\", \"Every\", \"Everyone\",\n",
    "    \"Everything\", \"Everywhere\", \"Except\", \"Few\", \"Fifteen\", \"Fifty\", \"Fill\",\n",
    "    \"Find\", \"Fire\", \"First\", \"Five\", \"For\", \"Former\", \"Formerly\", \"Forty\",\n",
    "    \"Found\", \"Four\", \"From\", \"Front\", \"Full\", \"Further\", \"get\", \"give\", \"go\",\n",
    "    \"Had\", \"Has\", \"Hasnt\", \"Have\", \"He\", \"Hence\", \"Her\", \"Here\", \"Hereafter\",\n",
    "    \"Hereby\", \"Herein\", \"Hereupon\", \"Hers\", \"Herself\", \"Him\", \"Himself\", \"His\",\n",
    "    \"How\", \"However\", \"Hundred\", \"I\", \"Ie\", \"If\", \"In\", \"Inc\", \"Indeed\",\n",
    "    \"Interest\", \"Into\", \"Is\", \"It\", \"Its\", \"Itself\", \"Keep\", \"Last\", \"Latter\",\n",
    "    \"Latterly\", \"Least\", \"Less\", \"Ltd\", \"Made\", \"Many\", \"May\", \"Me\",\n",
    "    \"Meanwhile\", \"Might\", \"Mill\", \"Mine\", \"More\", \"Moreover\", \"Most\", \"Mostly\",\n",
    "    \"Move\", \"Much\", \"Must\", \"My\", \"Myself\", \"Name\", \"Namely\", \"Neither\",\n",
    "    \"Never\", \"Nevertheless\", \"Next\", \"Nine\", \"No\", \"Nobody\", \"None\", \"Noone\",\n",
    "    \"Nor\", \"Not\", \"Nothing\", \"Now\", \"Nowhere\", \"Of\", \"Off\", \"Often\", \"On\",\n",
    "    \"Once\", \"One\", \"Only\", \"Onto\", \"Or\", \"Other\", \"Others\", \"Otherwise\", \"Our\",\n",
    "    \"Ours\", \"Ourselves\", \"Out\", \"Over\", \"Own\", \"Part\", \"Per\", \"Perhaps\",\n",
    "    \"Please\", \"Put\", \"Rather\", \"Re\", \"Same\", \"See\", \"Seem\", \"Seemed\",\n",
    "    \"Seeming\", \"Seems\", \"Serious\", \"Several\", \"She\", \"Should\", \"Show\", \"Side\",\n",
    "    \"Since\", \"Sincere\", \"Six\", \"Sixty\", \"So\", \"Some\", \"Somehow\", \"Someone\",\n",
    "    \"Something\", \"Sometime\", \"SometimeS\", \"Somewhere\", \"Still\", \"Such\",\n",
    "    \"System\", \"Take\", \"Ten\", \"Than\", \"That\", \"The\", \"Their\", \"Them\",\n",
    "    \"Themselves\", \"Then\", \"Thence\", \"There\", \"Thereafter\", \"Thereby\",\n",
    "    \"Therefore\", \"Therein\", \"Thereupon\", \"These\", \"They\", \"Thick\", \"Thin\",\n",
    "    \"Third\", \"This\", \"Those\", \"Though\", \"Three\", \"Through\", \"Throughout\",\n",
    "    \"Thru\", \"Thus\", \"To\", \"Together\", \"Too\", \"Top\", \"Toward\", \"Towards\",\n",
    "    \"Twelve\", \"Twenty\", \"Two\", \"Un\", \"Under\", \"Until\", \"Up\", \"Upon\", \"Us\",\n",
    "    \"Very\", \"Via\", \"Was\", \"We\", \"Well\", \"Were\", \"What\", \"Whatever\", \"When\",\n",
    "    \"Whence\", \"Whenever\", \"Where\", \"Whereafter\", \"Whereas\", \"Whereby\",\n",
    "    \"Wherein\", \"Whereupon\", \"Wherever\", \"Whether\", \"Which\", \"While\", \"Whither\",\n",
    "    \"Who\", \"Whoever\", \"Whole\", \"Whom\", \"Whose\", \"Why\", \"Will\", \"With\",\n",
    "    \"Within\", \"Without\", \"Would\", \"Yet\", \"You\", \"Your\", \"Yours\", \"Yourself\",\n",
    "    \"Yourselves\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a84d41d-8dd2-447d-ade4-543a41bf5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "\n",
    "# %% keywords split해서 하나로 합쳐주기\n",
    "\n",
    "def listsum(inputlist):\n",
    "    results = []\n",
    "    listlen = len(inputlist)\n",
    "    for i in range(listlen):\n",
    "        temp = inputlist[i].split(\" \")\n",
    "        for j in range(len(temp)):\n",
    "            if (len(temp[j]) > 2):\n",
    "                results.append(temp[j])\n",
    "\n",
    "    results = list(dict.fromkeys(results))\n",
    "    return results\n",
    "\n",
    "# %% 수정본 , rank 값이 1보다 큰 애들 적용\n",
    "\n",
    "def extractor(newsset, stop_words):\n",
    "    # stop_words = stopwords.words('english')\n",
    "    r1 = Rake(stopwords=stop_words)\n",
    "    # r2 = Rake()\n",
    "    title = newsset['Title']\n",
    "    title = preprocess_news(title)\n",
    "    news = newsset['text']\n",
    "    news = preprocess_news(news)\n",
    "    \n",
    "    # date = newsset['Date']\n",
    "    # keyword = []\n",
    "    r1.extract_keywords_from_text(title)\n",
    "    r1.get_ranked_phrases()\n",
    "    title_scores = r1.get_ranked_phrases_with_scores()\n",
    "\n",
    "    r1.extract_keywords_from_text(news)\n",
    "    r1.get_ranked_phrases()\n",
    "    text_scores = r1.get_ranked_phrases_with_scores()\n",
    "\n",
    "    # keyword.append(news_scores[i][1])\n",
    "    # keyword.append(title_scores[i][1])\n",
    "    title_tp = []\n",
    "    text_tp = []\n",
    "    new_text_scores = text_scores[:10]  # int(len(text_scores)/10)]\n",
    "    for data in new_text_scores:\n",
    "        text_tp.append(data[1])\n",
    "\n",
    "    for i in range(len(title_scores)):\n",
    "        if ((title_scores[i][0]) > 1):\n",
    "            title_tp.append(title_scores[i][1])\n",
    "\n",
    "    # return title_tp,text_tp\n",
    "    return listsum(title_tp), listsum(text_tp)\n",
    "\n",
    "# %% keyword extract from title, text no rank\n",
    "\n",
    "def extractor_norank(newsset, stop_words):\n",
    "    # stop_words = stopwords.words('english')\n",
    "    r1 = Rake(stopwords=stop_words)\n",
    "    # r2 = Rake()\n",
    "    title = newsset['Title']\n",
    "    # title = preprocess_news(title)\n",
    "    news = newsset['text']\n",
    "    # news = preprocess_news(news)\n",
    "\n",
    "    # date = newsset['Date']\n",
    "    # keyword = []\n",
    "    r1.extract_keywords_from_text(title)\n",
    "    r1.get_ranked_phrases()\n",
    "    title_scores = r1.get_ranked_phrases_with_scores()\n",
    "\n",
    "    r1.extract_keywords_from_text(news)\n",
    "    r1.get_ranked_phrases()\n",
    "    text_scores = r1.get_ranked_phrases_with_scores()\n",
    "    # keyword.append(news_scores[i][1])\n",
    "    # keyword.append(title_scores[i][1])\n",
    "    \n",
    "    title_tp = []\n",
    "    text_tp = []\n",
    "    \n",
    "#     print(\"start\")\n",
    "    for i in range(len(title_scores)):\n",
    "\n",
    "# # 오류났을 떼\n",
    "#         print(text_scores[i][1])\n",
    "        \n",
    "        \n",
    "        # 랭킹 메기는 것 없앰\n",
    "        title_tp.append(title_scores[i][1])\n",
    "        text_tp.append(text_scores[i][1])\n",
    "\n",
    "    # return title_tp,text_tp\n",
    "#     print(\"finish\\n\")\n",
    "    return listsum(title_tp), listsum(text_tp)\n",
    "\n",
    "# %%\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "                      u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                      u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                      u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                      u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                      u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U000024C2-\\U0001F251\"\n",
    "                      u\"\\U0001f926-\\U0001f937\"\n",
    "                      u\"\\U00010000-\\U0010ffff\"\n",
    "                      u\"\\u2640-\\u2642\"\n",
    "                      u\"\\u2600-\\u2B55\"\n",
    "                      u\"\\u200d\"\n",
    "                      u\"\\u23cf\"\n",
    "                      u\"\\u23e9\"\n",
    "                      u\"\\u231a\"\n",
    "                      u\"\\ufe0f\"  # dingbats\n",
    "                      u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "\n",
    "# %%\n",
    "def preprocess_tweet(text):\n",
    "    # convert text to lower-case\n",
    "    nopunc = text.lower()\n",
    "    nopunc = re.sub(\"\\\\’\", \"'\", nopunc)\n",
    "    nopunc = re.sub(\"\\\\'s\", \"\", nopunc)\n",
    "    nopunc = re.sub(\"\\\\'d\", \"\", nopunc)\n",
    "    nopunc = re.sub(\"\\\\'m\", \"\", nopunc)\n",
    "    nopunc = re.sub(\"\\\\'ve\", \"\", nopunc)\n",
    "    nopunc = re.sub(\"\\\\'re\", \"\", nopunc)\n",
    "\n",
    "    # remove URLs\n",
    "    nopunc = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', nopunc)\n",
    "    nopunc = re.sub(r'http\\S+', '', nopunc)\n",
    "    # remove usernames\n",
    "    nopunc = re.sub('@[^\\s]+', '', nopunc)\n",
    "    # remove the # in #hashtag\n",
    "    nopunc = re.sub(r'#([^\\s]+)', r'\\1', nopunc)\n",
    "    # remove rt\n",
    "    nopunc = re.sub('^(rt )', '', nopunc)\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = re.sub(\"  \", \" \", nopunc)\n",
    "\n",
    "    # \\'를 제외했음, it's 같은 '도 생략되어버림.\n",
    "    punctuations = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    result = [char for char in nopunc if char not in punctuations]\n",
    "    # Join the characters again to form the string.\n",
    "    result = ''.join(result)\n",
    "\n",
    "    result = remove_emojis(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "# %%\n",
    "def preprocess_news(inputnews):\n",
    "    inputnews = inputnews.lower()\n",
    "    inputnews = re.sub(\"\\\\'s\", \"\", inputnews)\n",
    "    inputnews = re.sub(\"\\\\'d\", \"\", inputnews)\n",
    "    inputnews = re.sub(\"\\\\'m\", \"\", inputnews)\n",
    "    inputnews = re.sub(\"\\\\'ve\", \"\", inputnews)\n",
    "    inputnews = re.sub(\"\\\\'re\", \"\", inputnews)\n",
    "\n",
    "    inputnews = [char for char in inputnews if char not in string.punctuation]\n",
    "    # Join the characters again to form the string.\n",
    "    inputnews = ''.join(inputnews)\n",
    "\n",
    "    return inputnews\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "def is_word_in_text(word, text):\n",
    "    \"\"\"\n",
    "    Check if a word is in a text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word : str\n",
    "    text : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool : True if word is in text, otherwise False.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> is_word_in_text(\"Python\", \"python is awesome.\")\n",
    "    True\n",
    "\n",
    "    >>> is_word_in_text(\"Python\", \"camelCase is pythonic.\")\n",
    "    False\n",
    "\n",
    "    >>> is_word_in_text(\"Python\", \"At the end is Python\")\n",
    "    True\n",
    "    \"\"\"\n",
    "    pattern = r'(^|[^\\w]){}([^\\w]|$)'.format(word)\n",
    "    pattern = re.compile(pattern, re.IGNORECASE)\n",
    "    matches = re.search(pattern, text)\n",
    "    return bool(matches)\n",
    "\n",
    "#%%\n",
    "def tf_vectorizer(filtered_news, filtered_tw, my_stop_words):\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words = set(my_stop_words))\n",
    "    \n",
    "   \n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([filtered_news,filtered_tw])\n",
    "    return cosine_similarity(tfidf_matrix[0],tfidf_matrix[1])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62db1027-4b92-4eb9-b4bb-7315232e697d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet city name? Boston\n",
      "news city name? Boston\n"
     ]
    }
   ],
   "source": [
    "# load news json file\n",
    "tweet_city_name = input(\"tweet city name? \")\n",
    "city_name = input(\"news city name? \")\n",
    "# %%\n",
    "# with open('데이터수집.전처리코드/Data/CNNtest_9_Boston2018.json', encoding=\"UTF-8\") as newsfile:\n",
    "with open('/Users/jinyuntae/Desktop/Personal_project/estimate_location_with_tweet/data/news_data/2_result_data/2_2018_CNNtest_{}.json'.format(city_name), encoding=\"UTF-8\") as newsfile:\n",
    "    newsfile = newsfile.read()\n",
    "    news_data = json.loads(newsfile)\n",
    "    newslen = len(news_data['news'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ce2ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directory\n",
    "if os.path.isdir(\"../data/analysis_data/2018_{}2{}\".format(tweet_city_name, city_name)) == False:\n",
    "    os.mkdir(\"../data/analysis_data/2018_{}2{}\".format(tweet_city_name, city_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b70d0b35-facd-4f7a-981f-c0344fcd279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% without split\n",
    "resultpd = pd.DataFrame(columns=[\"Text\", \"Date\"])\n",
    "\n",
    "for i in range(newslen):\n",
    "    title_norank, text_norank = (extractor_norank(news_data[\"news\"][i], stop_words))\n",
    "    title_result, text_result = (extractor(news_data[\"news\"][i], stop_words))\n",
    "    date = news_data[\"news\"][i]['Date']\n",
    "    tempdf = pd.DataFrame(\n",
    "        {\"ExtTitle_no\": [title_norank], 'ExtText_no': [text_norank], \"ExtTitle\": [title_result], \"Date\": date,\n",
    "         'ExtText': [text_result], \"Title\": news_data[\"news\"][i][\"Title\"], \"Text\": news_data['news'][i]['text']})\n",
    "\n",
    "    resultpd = resultpd.append(tempdf, ignore_index=True)\n",
    "\n",
    "# newsmerge = resultpd.groupby([\"Date\"]).agg({\"Title\":'sum'}).reset_index()\n",
    "\n",
    "# %% 상위 10개, 전처리 후\n",
    "resultpd1 = pd.DataFrame(columns=[\"Text\", \"Date\"])\n",
    "\n",
    "for i in range(newslen):\n",
    "    title_norank, text_norank = (extractor_norank(news_data[\"news\"][i], stop_words))\n",
    "    title_result, text_result = (extractor(news_data[\"news\"][i], stop_words))\n",
    "    date = news_data[\"news\"][i]['Date']\n",
    "    tempdf = pd.DataFrame(\n",
    "        {\"ExtTitle_no\": [title_norank], 'ExtText_no': [text_norank], \"ExtTitle\": [title_result], \"Date\": date,\n",
    "         'ExtText': [text_result], \"Title\": news_data[\"news\"][i][\"Title\"], \"Text\": news_data['news'][i]['text']})\n",
    "\n",
    "    resultpd1 = resultpd1.append(tempdf, ignore_index=True)\n",
    "\n",
    "# newsmerge = resultpd.groupby([\"Date\"]).agg({\"Title\":'sum'}).reset_index()\n",
    "# %% 상위 10개, 전처리 전\n",
    "resultpd2 = pd.DataFrame(columns=[\"Text\", \"Date\"])\n",
    "\n",
    "for i in range(newslen):\n",
    "    title_norank, text_norank = (extractor_norank(news_data[\"news\"][i], stop_words))\n",
    "    title_result, text_result = (extractor(news_data[\"news\"][i], stop_words))\n",
    "    date = news_data[\"news\"][i]['Date']\n",
    "    tempdf = pd.DataFrame(\n",
    "        {\"ExtTitle_no\": [title_norank], 'ExtText_no': [text_norank], \"ExtTitle\": [title_result], \"Date\": date,\n",
    "         'ExtText': [text_result], \"Title\": news_data[\"news\"][i][\"Title\"], \"Text\": news_data['news'][i]['text']})\n",
    "\n",
    "    resultpd2 = resultpd2.append(tempdf, ignore_index=True)\n",
    "\n",
    "# %% 상위 10개, 전처리 후, 샌프란\n",
    "resultpd3 = pd.DataFrame(columns=[\"Text\", \"Date\"])\n",
    "\n",
    "for i in range(newslen):\n",
    "    title_norank, text_norank = (extractor_norank(news_data[\"news\"][i], stop_words))\n",
    "    title_result, text_result = (extractor(news_data[\"news\"][i], stop_words))\n",
    "    date = news_data[\"news\"][i]['Date']\n",
    "    tempdf = pd.DataFrame(\n",
    "        {\"ExtTitle_no\": [title_norank], 'ExtText_no': [text_norank], \"ExtTitle\": [title_result], \"Date\": date,\n",
    "         'ExtText': [text_result], \"Title\": news_data[\"news\"][i][\"Title\"], \"Text\": news_data['news'][i]['text']})\n",
    "\n",
    "    resultpd3 = resultpd3.append(tempdf, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "712fe791-cb6a-4d18-b7c0-9ee323ab80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# newsmerge = resultpd.groupby([\"Date\"]).agg({\"Title\":'sum'}).reset_index()\n",
    "# %%\n",
    "# with open('C:/Users/cowgi/Desktop/University/bigbaselab/PredictLocation_TweetAndNews/boston_tweet_more_150_everymonth_golbang.json', encoding=\"UTF-8\") as twfile:\n",
    "with open('/Users/jinyuntae/Desktop/Personal_project/estimate_location_with_tweet/data/tweet_data/5_result_data/{}_user_filtered_count0.json'.format(tweet_city_name), encoding=\"UTF-8\") as twfile:\n",
    "    twfile = twfile.read()\n",
    "    tw_data = json.loads(twfile)\n",
    "    usernum = len(tw_data['users'])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ced8f08-c0ef-4cfb-b671-49aed7c25290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "wnews = []\n",
    "wtweets = []\n",
    "countlist = []\n",
    "countlist_no = []\n",
    "keywordlist = []\n",
    "keywordlist_no = []\n",
    "wkeys = []\n",
    "wkeys_no = []\n",
    "unions = []\n",
    "\n",
    "# with open('C:/Users/cowgi/Desktop/University/bigbaselab/PredictLocation_TweetAndNews/CNN_result/join_tweet/200708_1.json', 'a' ,encoding='UTF-8') as tf:\n",
    "#    sys.stdout = tf\n",
    "for t in range(usernum):\n",
    "    try:\n",
    "        eachuser = tw_data['users'][t]\n",
    "        username = eachuser['id']\n",
    "        usertimeline = eachuser['timeline']\n",
    "        # count = 0\n",
    "\n",
    "        # datecount = 0\n",
    "        # print(\"{\\\"id : \\\"%s\\\":[\"%(username))\n",
    "        for i in range(len(usertimeline)):\n",
    "            date = usertimeline[i][\"date\"]\n",
    "            text = preprocess_tweet(usertimeline[i][\"text\"])\n",
    "            # text = usertimeline[i][\"text\"]\n",
    "\n",
    "            for j in range(len(resultpd3)):\n",
    "                if (date == resultpd3._get_value(j, \"Date\")):\n",
    "                    count = 0\n",
    "                    count_no = 0\n",
    "                    keywords = []\n",
    "                    keywords_no = []\n",
    "                    for word in resultpd3._get_value(j, \"ExtText\"):\n",
    "                        if is_word_in_text(word, text):\n",
    "                            count += 1\n",
    "                            keywords.append(word)\n",
    "                    for word in resultpd3._get_value(j, \"ExtText_no\"):\n",
    "                        if is_word_in_text(word, text):\n",
    "                            count_no += 1\n",
    "                            keywords_no.append(word)\n",
    "\n",
    "                    #  n(A)+n(B)\n",
    "                    unions.append(len(text.split()) + len(resultpd3._get_value(j, \"ExtText\")))\n",
    "                    wnews.append(resultpd3._get_value(j, \"Title\"))\n",
    "                    wtweets.append(text)\n",
    "                    countlist.append(count)\n",
    "                    countlist_no.append(count_no)\n",
    "                    keywordlist.append(keywords)\n",
    "                    keywordlist_no.append(keywords_no)\n",
    "                    wkeys.append(resultpd3._get_value(j, \"ExtText\"))\n",
    "                    wkeys_no.append(resultpd3._get_value(j, \"ExtText_no\"))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0f22739-1f2f-47a7-bae9-69f5afc1c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    " \n",
    "Toppd = pd.DataFrame()\n",
    " \n",
    "Toppd[\"NewsTitle\"] = wnews\n",
    "Toppd[\"Tweet\"] = wtweets\n",
    "Toppd[\"howmany\"] = countlist\n",
    "Toppd[\"matched_kewords\"] = keywordlist\n",
    "Toppd[\"Extkey\"] = wkeys\n",
    "Toppd[\"howmany_norank\"] = countlist_no\n",
    "Toppd[\"matched_kewords_norank\"] = keywordlist_no\n",
    "Toppd[\"Extkey_norank\"] = wkeys_no\n",
    "Toppd[\"Union\"] = unions\n",
    "tmp = []\n",
    "tmp_no = []\n",
    "for i in range(len(Toppd[\"Extkey\"])):\n",
    "    tmp.append(len(Toppd[\"Extkey\"][i]))\n",
    "    tmp_no.append(len(Toppd[\"Extkey_norank\"][i]))\n",
    "Toppd[\"numKeys\"] = tmp\n",
    "Toppd[\"numKeys_norank\"] = tmp_no\n",
    "Toppd[\"ratio\"] = Toppd[\"howmany\"]/Toppd[\"numKeys\"]\n",
    "Toppd[\"ratio_norank\"] = Toppd[\"howmany_norank\"]/Toppd[\"numKeys_norank\"]\n",
    "Toppdresult = Toppd\n",
    "#finalresult = Toppdresult.sort_values(by=[\"howmany_norank\",\"numKeys_norank\"],ascending=[False,True])\n",
    "finalresult = Toppdresult.sort_values(by=[\"howmany\",\"Union\"],ascending=[False,True])\n",
    "#finalresult = finalresult.sort_values(by=[\"numKeys_norank\"],ascending=True)\n",
    "finalresult = finalresult.reset_index(drop=True)\n",
    "finalresult = finalresult[:200]\n",
    " \n",
    "#finalresult.to_excel(\"C:/Users/cowgi/Desktop/University/bigbaselab/PredictLocation_TweetAndNews/CNN_result/join_tweet/keywordTop200_sanfransisco.xlsx\")\n",
    "#finalresult.to_excel(\"C:/Users/cowgi/Desktop/University/bigbaselab/keywordTop200_sort_top10rank_nopre.xlsx\")\n",
    "#finalresult.to_excel(\"C:/Users/cowgi/Desktop/University/bigbaselab/keywordTop200_sort_top10rank_befpre.xlsx\")\n",
    "finalresult.to_excel(\"/Users/jinyuntae/Desktop/Personal_project/estimate_location_with_tweet/data/analysis_data/2018_{}2{}/keywordTop200_sort_top10rank_aftpre_{}2{}.xlsx\".format(tweet_city_name, city_name, tweet_city_name, city_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1f11a77-bbd3-44dd-aae7-37fc9c86bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "tf_wnews = []\n",
    "tf_wtweets = []\n",
    "tf_similarity = []\n",
    "tf_date = []\n",
    "for t in range(usernum):\n",
    "    try:\n",
    "        eachuser = tw_data['users'][t]\n",
    "        username = eachuser['id']\n",
    "        usertimeline = eachuser['timeline']\n",
    "        #count = 0\n",
    "        datecount = 0\n",
    "        \n",
    "        for i in range(len(usertimeline)):\n",
    "            date = usertimeline[i][\"date\"]\n",
    "            text = preprocess_tweet(usertimeline[i][\"text\"])\n",
    "            \n",
    "            for j in range(len(resultpd)):\n",
    "        \n",
    "                if(date == resultpd._get_value(j,\"Date\")):\n",
    "                    newstext = resultpd._get_value(j,\"Text\")\n",
    "                    newstext = preprocess_news(newstext)\n",
    "                    \n",
    "                    tf_wnews.append(resultpd._get_value(j,\"Title\"))\n",
    "                    tf_wtweets.append(text)\n",
    "                    tf_date.append(date)\n",
    "                    tf_similarity.append(tf_vectorizer(text, newstext, stop_words))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e86b4dd6-1dd4-4ee1-9ed8-705fff45eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    " \n",
    "tf_Toppd = pd.DataFrame()\n",
    " \n",
    "tf_Toppd[\"NewsTitle\"] = tf_wnews\n",
    "tf_Toppd[\"Tweet\"] = tf_wtweets\n",
    "tf_Toppd[\"Date\"] = tf_date\n",
    "tf_Toppd[\"similarity\"] = tf_similarity\n",
    " \n",
    "tf_result = tf_Toppd\n",
    "tf_result = tf_result.sort_values(by=[\"similarity\"],ascending=False)\n",
    "tf_result = tf_result.reset_index(drop=True)\n",
    "tf_result = tf_result[:200]\n",
    " \n",
    "tf_result.to_excel(\"/Users/jinyuntae/Desktop/Personal_project/estimate_location_with_tweet/data/analysis_data/2018_{}2{}/TF_IDF_Top200_{}2{}.xlsx\".format(tweet_city_name, city_name, tweet_city_name, city_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eef1a0ac-af9f-4b94-bf54-46b47e87c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% DocSim 활용하여 word2vec 모델 사용\n",
    "import gensim\n",
    " \n",
    "pre_model = gensim.models.KeyedVectors.load_word2vec_format('/Users/jinyuntae/Desktop/Personal_project/estimate_location_with_tweet/model/GoogleNews-vectors-negative300.bin', binary = True) \n",
    " \n",
    "sys.path.append(\"/Users/jinyuntae/Desktop/Personal_project/estimate_location_with_tweet/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "accfe571-0901-4c02-aa66-8735dd713296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "from DocSim import DocSim\n",
    "ds = DocSim(pre_model,stopwords = stop_words)\n",
    " \n",
    "#%%\n",
    "w2v_wnews = []\n",
    "w2v_wtweets = []\n",
    "w2v_similarity = []\n",
    "w2v_date = []\n",
    "\n",
    "for t in range(usernum):\n",
    "    eachuser = tw_data['users'][t]\n",
    "    username = eachuser['id']\n",
    "    usertimeline = eachuser['timeline']\n",
    "    \n",
    "    datecount = 0\n",
    "    \n",
    "    for i in range(len(usertimeline)):\n",
    "        date = usertimeline[i][\"date\"]\n",
    "        text = preprocess_tweet(usertimeline[i][\"text\"])\n",
    "        \n",
    "        for j in range(len(resultpd)):\n",
    "            if (date == resultpd._get_value(j, \"Date\")):\n",
    "                try:\n",
    "                    newstext = resultpd._get_value(j, \"Text\")\n",
    "                    newstext = preprocess_news(newstext)\n",
    "                    text_scores = ds.calculate_similarity(text, [newstext])\n",
    "                    w2v_similarity.append(text_scores[0][\"score\"])\n",
    "                    w2v_wnews.append(resultpd._get_value(j, \"Title\"))\n",
    "                    w2v_wtweets.append(text)\n",
    "                    w2v_date.append(date)\n",
    "                    \n",
    "                    \n",
    "                except:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65503562-6821-4c21-877e-c09f8aac4d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%           \n",
    "w2v_Toppd = pd.DataFrame()\n",
    " \n",
    "w2v_Toppd[\"NewsTitle\"] = w2v_wnews\n",
    "w2v_Toppd[\"Tweet\"] = w2v_wtweets\n",
    "w2v_Toppd[\"Date\"] = w2v_date\n",
    "w2v_Toppd[\"similarity\"] = w2v_similarity\n",
    " \n",
    "w2v_result = w2v_Toppd\n",
    "w2v_result = w2v_result.sort_values(by=[\"similarity\"],ascending=False)\n",
    "w2v_result = w2v_result.reset_index(drop=True)\n",
    "w2v_result = w2v_result[:200]\n",
    " \n",
    "w2v_result.to_excel(\"/Users/jinyuntae/Desktop/Personal_project/estimate_location_with_tweet/data/analysis_data/2018_{}2{}/Word2Vec_Top200_{}2{}.xlsx\".format(tweet_city_name, city_name, tweet_city_name, city_name))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f400fe86-6de9-45f8-ac83-f951a448d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%  Doc2Vec pre모델 사용\n",
    "from gensim.models import Doc2Vec\n",
    "from scipy import spatial\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "d2v_apnews_model = Doc2Vec.load(\"/Users/jinyuntae/Desktop/Personal_project/estimate_location_with_tweet/model/doc2vec_model/doc2vec.bin\")\n",
    "d2v_enwiki_model = Doc2Vec.load(\"/Users/jinyuntae/Desktop/Personal_project/estimate_location_with_tweet/model/doc2vec_model/doc2vec.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4148893e-5931-4907-96c2-fea9fbc8909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "d2v_wnews = []\n",
    "d2v_wtweets = []\n",
    "d2v_ap_similarity = []\n",
    "# d2v_wiki_similarity = []\n",
    "d2v_date = []\n",
    "for t in range(usernum):\n",
    "\n",
    "    eachuser = tw_data['users'][t]\n",
    "    username = eachuser['id']\n",
    "    usertimeline = eachuser['timeline']\n",
    "    # count = 0\n",
    "    datecount = 0\n",
    "\n",
    "    for i in range(len(usertimeline)):\n",
    "        date = usertimeline[i][\"date\"]\n",
    "        text = preprocess_tweet(usertimeline[i][\"text\"])\n",
    "        for j in range(len(resultpd)):\n",
    "            if (date == resultpd._get_value(j, \"Date\")):\n",
    "                try:\n",
    "                    first_text = resultpd._get_value(j, \"Text\")\n",
    "                    first_text = preprocess_news(first_text)\n",
    "                    Title = resultpd._get_value(j, \"Title\")\n",
    "                    second_text = text\n",
    "\n",
    "                    vec1 = d2v_apnews_model.infer_vector(first_text.split())\n",
    "                    vec2 = d2v_apnews_model.infer_vector(second_text.split())\n",
    "\n",
    "#                     vec3 = d2v_enwiki_model.infer_vector(first_text.split())\n",
    "#                     vec4 = d2v_enwiki_model.infer_vector(second_text.split())\n",
    "\n",
    "                    # 이거 distnace로 안하고 그냥 tf-idf에서 사용헌 cosinesimilarity 써도 될듯.\n",
    "                    text_scores_ap = spatial.distance.cosine(vec1, vec2)\n",
    "#                     text_scores_wiki = spatial.distance.cosine(vec3, vec4)\n",
    "\n",
    "                    d2v_ap_similarity.append(1 - text_scores_ap)\n",
    "#                     d2v_wiki_similarity.append(1 - text_scores_wiki)\n",
    "                    d2v_wnews.append(Title)\n",
    "                    d2v_wtweets.append(second_text)\n",
    "                    d2v_date.append(date)\n",
    "\n",
    "                except:\n",
    "                    print(\"empty\")\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2959f8c-c033-45ff-95f0-de9db4e031fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_Toppd = pd.DataFrame()\n",
    "\n",
    "d2v_Toppd[\"NewsTitle\"] = d2v_wnews\n",
    "d2v_Toppd[\"Tweet\"] = d2v_wtweets\n",
    "d2v_Toppd[\"Date\"] = d2v_date\n",
    "d2v_Toppd[\"ap_similarity\"] = d2v_ap_similarity\n",
    "# d2v_Toppd[\"wiki_similarity\"] = d2v_wiki_similarity\n",
    "\n",
    "d2v_ap_result = d2v_Toppd\n",
    "# d2v_wiki_result = d2v_Toppd\n",
    "d2v_ap_result = d2v_ap_result.sort_values(by=[\"ap_similarity\"], ascending=False)\n",
    "# d2v_wiki_result = d2v_wiki_result.sort_values(by=[\"wiki_similarity\"], ascending=False)\n",
    "\n",
    "d2v_ap_result = d2v_ap_result.reset_index(drop=True)\n",
    "# d2v_wiki_result = d2v_wiki_result.reset_index(drop=True)\n",
    "\n",
    "d2v_ap_result = d2v_ap_result[:200]\n",
    "# d2v_wiki_result = d2v_wiki_result[:200]\n",
    "\n",
    "d2v_ap_result.to_excel(\"/Users/jinyuntae/Desktop/Personal_project/estimate_location_with_tweet/data/analysis_data/2018_{}2{}/Doc2Vec_ap_Top200_{}2{}.xlsx\".format(tweet_city_name, city_name, tweet_city_name, city_name))\n",
    "# d2v_wiki_result.to_excel(\"/Users/jinyuntae/Desktop/Personal_project/estimate_location_with_tweet/data/analysis_data/2018_{}2{}/Doc2Vec_wiki_Top200_{}2{}.xlsx\".format(tweet_city_name, city_name, tweet_city_name, city_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
